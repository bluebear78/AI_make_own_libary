{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SGD:\n",
    "    def __init__(self, fit_intercept=True, copy_X=True,\n",
    "                 alpha=0.0001, epochs=100000, weight_decay=0.9,batch_size=1,shuffle=True):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.copy_X = copy_X\n",
    "        self._alpha = alpha\n",
    "        self._epochs = epochs\n",
    "        self._BATCH_SIZE = batch_size\n",
    "        self._cost_history = []\n",
    "\n",
    "        self._coef = None\n",
    "        self._intercept = None\n",
    "        self._new_X = None\n",
    "        self._w_history = None\n",
    "        self._weight_decay = weight_decay\n",
    "\n",
    "    def cost(self, h, y):\n",
    "        m = y.size\n",
    "        predictions = h\n",
    "        sqErrors = (predictions - y)\n",
    "\n",
    "        J = (1.0/(2*m)) * sqErrors.T.dot(sqErrors)\n",
    "        return J\n",
    "\n",
    "    def hypothesis_function(self, X, theta):\n",
    "        return X.dot(theta)\n",
    "\n",
    "    def gradient(self, X, theta):\n",
    "        predictions = X.dot(theta)\n",
    "        m = X.shape[0]\n",
    "        for i in range(theta.size):\n",
    "            partial_marginal = X[:,i]\n",
    "            errors_xi = (predictions - y) * partial_marginal\n",
    "            theta[i] = theta[i] - self._alpha * (1.0/m) * errors_xi.sum()\n",
    "        return theta\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        if self.fit_intercept == True:\n",
    "            make_ones = k = np.ones([X.shape[0],1])\n",
    "            np.hstack((make_ones,X))\n",
    "        \n",
    "        cost_history = []\n",
    "        theta_history = []\n",
    "\n",
    "        theta = np.random.normal((X.shape[0],1))\n",
    "        \n",
    "        for _ in range(self._epochs):\n",
    "            self.X_copy = np.copy(X)\n",
    "            \n",
    "            if self.is_SGD:\n",
    "                shuffle_index = np.random.shuffle(self.X_copy.shape[0])\n",
    "            batch = len(self.X_copy) // self._BATCH_SIZE\n",
    "            \n",
    "            for batch_count in range(batch):\n",
    "                X_batch = np.copy(X_copy[batch_count*self.BATCH_SIZE : (batch_count+1)*BATCH_SIZE])\n",
    "                theta = self.gradient(X_batch,theta)#theta값 수정 필요...\n",
    "            print(\"Number of epoch : %d\" %epoch)\n",
    "            if _ % 1000 == 0:\n",
    "                theta_history.append(theta)\n",
    "                cost_history.append(self.cost(self.hypothesis_function(X,theta),y))\n",
    "        self._coef = theta\n",
    "        return theta,np.array(cost_history),np.array(theta_history)\n",
    "\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        return X.dot(self.coef)\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def coef(self):\n",
    "        return self._coef\n",
    "\n",
    "    @property\n",
    "    def intercept(self):\n",
    "        return self._intercept\n",
    "\n",
    "    @property\n",
    "    def weights_history(self):\n",
    "        return np.array(self._w_history)\n",
    "\n",
    "    @property\n",
    "    def cost_history(self):\n",
    "        return self._cost_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_regression(n_samples=1000,n_features=1,noise=10,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_lr = SGD(alpha=0.001,epochs=10000,batch_size=10,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1,2],[2,4],[5,8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.random.normal((X.shape[0],1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [2, 4],\n",
       "       [5, 8]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.3294799 ,  4.65895981, 11.4630002 ])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dot(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.14508058, 0.09219966])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 5])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array([4,8,12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hypp = X.dot(theta) - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.6705201 , -3.34104019, -0.5369998 ])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X.dot(theta) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.6705201 , -3.34104019, -0.5369998 ])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X.dot(theta) - y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [1, 2],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1,2],[1,2],[1,2]])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.0554856, -0.1109712])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.01*hypp.dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.14508058, 0.09219966])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.1469301, 0.0958987])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta - 0.001*hypp.dot(X)*(1.0/X.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, fit_intercept=True, copy_X=True,\n",
    "                 alpha=0.0001, epochs=1000000, weight_decay=0.9):\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.copy_X = copy_X\n",
    "        self._alpha = alpha\n",
    "        self._epochs = epochs\n",
    "\n",
    "        self._cost_history = []\n",
    "\n",
    "        self._coef = None\n",
    "        self._intercept = None\n",
    "        self._new_X = None\n",
    "        self._w_history = None\n",
    "        self._weight_decay = weight_decay\n",
    "\n",
    "    def cost(self, h, y):\n",
    "        m = y.size\n",
    "        predictions = h\n",
    "        sqErrors = (predictions - y)\n",
    "\n",
    "        J = (1.0/(2*m)) * sqErrors.T.dot(sqErrors)\n",
    "        return J\n",
    "\n",
    "    def hypothesis_function(self, X, theta):\n",
    "        return X.dot(theta)\n",
    "\n",
    "    def gradient(self, X, theta):\n",
    "        predictions = X.dot(theta)\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        hypo = (predictions-y).dot(X)\n",
    "        theta = theta - self._alpha * (1.0/m) * hypo\n",
    "        \n",
    "        #for i in range(theta.size):\n",
    "        #    partial_marginal = X[:,i]\n",
    "        #    errors_xi = (predictions - y) * partial_marginal\n",
    "        #    theta[i] = theta[i] - self._alpha * (1.0/m) * errors_xi.sum()\n",
    "        \n",
    "        return theta\n",
    "    \n",
    "    def fit(self,X,y):\n",
    "        if self.fit_intercept == True:\n",
    "            make_ones = k = np.ones([X.shape[0],1])\n",
    "            np.hstack((make_ones,X))\n",
    "        \n",
    "        cost_history = []\n",
    "        theta_history = []\n",
    "\n",
    "        theta = np.random.normal((X.shape[0],1))\n",
    "        \n",
    "        for _ in range(self._epochs):\n",
    "            theta = self.gradient(X,theta)\n",
    "            \n",
    "            if _ % 1000 == 0:\n",
    "                theta_history.append(theta)\n",
    "                cost_history.append(self.cost(self.hypothesis_function(X,theta),y))\n",
    "        \n",
    "        self._coef = theta\n",
    "        return theta,np.array(cost_history),np.array(theta_history)\n",
    "\n",
    "        \n",
    "        \n",
    "    def predict(self, X):\n",
    "        return X.dot(self.coef)\n",
    "\n",
    "    @property\n",
    "    def coef(self):\n",
    "        return self._coef\n",
    "\n",
    "    @property\n",
    "    def intercept(self):\n",
    "        return self._intercept\n",
    "\n",
    "    @property\n",
    "    def weights_history(self):\n",
    "        return np.array(self._w_history)\n",
    "\n",
    "    @property\n",
    "    def cost_history(self):\n",
    "        return self._cost_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X = np.array([[1,2],[1,4],[1,5]])\n",
    "    y = np.array([4,8,12])\n",
    "    my_sgd = SGD(fit_intercept=True)\n",
    "    my_sgd.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-1.42833202,  2.57136967]),\n",
       " array([7.82420537, 1.38306122, 1.09902266, 1.07060154, 1.0531014 ,\n",
       "        1.03637028, 1.01998115, 1.00391028, 0.98815079, 0.97269663,\n",
       "        0.95754188, 0.94268074, 0.92810752, 0.91381664, 0.89980263,\n",
       "        0.88606013, 0.87258388, 0.85936871, 0.84640958, 0.83370152,\n",
       "        0.82123966, 0.80901924, 0.79703558, 0.78528409, 0.77376027,\n",
       "        0.76245972, 0.7513781 , 0.74051118, 0.72985479, 0.71940486,\n",
       "        0.70915739, 0.69910846, 0.68925421, 0.67959087, 0.67011476,\n",
       "        0.66082224, 0.65170974, 0.6427738 , 0.63401098, 0.62541793,\n",
       "        0.61699136, 0.60872805, 0.60062483, 0.59267861, 0.58488633,\n",
       "        0.57724502, 0.56975176, 0.56240367, 0.55519794, 0.54813182,\n",
       "        0.54120259, 0.53440761, 0.52774428, 0.52121004, 0.5148024 ,\n",
       "        0.5085189 , 0.50235714, 0.49631475, 0.49038943, 0.48457891,\n",
       "        0.47888096, 0.4732934 , 0.46781409, 0.46244095, 0.4571719 ,\n",
       "        0.45200493, 0.44693807, 0.44196937, 0.43709694, 0.43231891,\n",
       "        0.42763344, 0.42303875, 0.41853308, 0.41411471, 0.40978193,\n",
       "        0.4055331 , 0.40136658, 0.39728079, 0.39327415, 0.38934514,\n",
       "        0.38549225, 0.38171401, 0.37800897, 0.3743757 , 0.37081283,\n",
       "        0.36731899, 0.36389283, 0.36053306, 0.35723837, 0.35400752,\n",
       "        0.35083926, 0.34773238, 0.3446857 , 0.34169804, 0.33876827,\n",
       "        0.33589526, 0.3330779 , 0.33031514, 0.3276059 , 0.32494914,\n",
       "        0.32234386, 0.31978906, 0.31728375, 0.31482697, 0.3124178 ,\n",
       "        0.3100553 , 0.30773857, 0.30546673, 0.3032389 , 0.30105424,\n",
       "        0.2989119 , 0.29681106, 0.29475093, 0.29273071, 0.29074963,\n",
       "        0.28880693, 0.28690187, 0.28503371, 0.28320176, 0.28140529,\n",
       "        0.27964363, 0.2779161 , 0.27622203, 0.27456079, 0.27293174,\n",
       "        0.27133424, 0.2697677 , 0.2682315 , 0.26672507, 0.26524782,\n",
       "        0.2637992 , 0.26237864, 0.2609856 , 0.25961955, 0.25827996,\n",
       "        0.25696633, 0.25567815, 0.25441493, 0.25317618, 0.25196143,\n",
       "        0.25077021, 0.24960207, 0.24845657, 0.24733325, 0.2462317 ,\n",
       "        0.2451515 , 0.24409222, 0.24305346, 0.24203483, 0.24103593,\n",
       "        0.24005638, 0.23909582, 0.23815386, 0.23723015, 0.23632434,\n",
       "        0.23543608, 0.23456502, 0.23371085, 0.23287322, 0.23205182,\n",
       "        0.23124633, 0.23045645, 0.22968187, 0.2289223 , 0.22817744,\n",
       "        0.22744702, 0.22673074, 0.22602835, 0.22533956, 0.22466412,\n",
       "        0.22400176, 0.22335223, 0.22271529, 0.22209069, 0.22147819,\n",
       "        0.22087756, 0.22028856, 0.21971098, 0.21914458, 0.21858916,\n",
       "        0.2180445 , 0.21751039, 0.21698663, 0.21647302, 0.21596935,\n",
       "        0.21547545, 0.21499111, 0.21451616, 0.21405041, 0.21359368,\n",
       "        0.2131458 , 0.2127066 , 0.21227591, 0.21185356, 0.2114394 ,\n",
       "        0.21103326, 0.21063498, 0.21024443, 0.20986144, 0.20948587,\n",
       "        0.20911757, 0.20875641, 0.20840225, 0.20805495, 0.20771438,\n",
       "        0.20738041, 0.20705291, 0.20673175, 0.20641681, 0.20610798,\n",
       "        0.20580513, 0.20550815, 0.20521692, 0.20493133, 0.20465127,\n",
       "        0.20437665, 0.20410734, 0.20384325, 0.20358428, 0.20333032,\n",
       "        0.20308128, 0.20283707, 0.20259759, 0.20236275, 0.20213246,\n",
       "        0.20190663, 0.20168518, 0.20146802, 0.20125506, 0.20104623,\n",
       "        0.20084145, 0.20064063, 0.2004437 , 0.20025059, 0.20006122,\n",
       "        0.19987552, 0.19969342, 0.19951485, 0.19933973, 0.19916801,\n",
       "        0.19899961, 0.19883448, 0.19867255, 0.19851375, 0.19835803,\n",
       "        0.19820533, 0.19805558, 0.19790874, 0.19776474, 0.19762353,\n",
       "        0.19748506, 0.19734927, 0.19721611, 0.19708553, 0.19695748,\n",
       "        0.19683191, 0.19670878, 0.19658803, 0.19646962, 0.1963535 ,\n",
       "        0.19623963, 0.19612797, 0.19601847, 0.1959111 , 0.1958058 ,\n",
       "        0.19570255, 0.19560129, 0.195502  , 0.19540463, 0.19530914,\n",
       "        0.19521551, 0.19512369, 0.19503365, 0.19494535, 0.19485877,\n",
       "        0.19477386, 0.1946906 , 0.19460895, 0.19452888, 0.19445036,\n",
       "        0.19437337, 0.19429786, 0.19422382, 0.19415121, 0.19408001,\n",
       "        0.19401019, 0.19394173, 0.19387458, 0.19380874, 0.19374418,\n",
       "        0.19368086, 0.19361878, 0.19355789, 0.19349819, 0.19343964,\n",
       "        0.19338223, 0.19332592, 0.19327071, 0.19321657, 0.19316348,\n",
       "        0.19311142, 0.19306036, 0.1930103 , 0.1929612 , 0.19291306,\n",
       "        0.19286584, 0.19281955, 0.19277415, 0.19272963, 0.19268597,\n",
       "        0.19264316, 0.19260117, 0.19256   , 0.19251963, 0.19248004,\n",
       "        0.19244122, 0.19240315, 0.19236582, 0.19232921, 0.19229331,\n",
       "        0.1922581 , 0.19222358, 0.19218972, 0.19215653, 0.19212397,\n",
       "        0.19209205, 0.19206074, 0.19203004, 0.19199994, 0.19197042,\n",
       "        0.19194147, 0.19191308, 0.19188524, 0.19185794, 0.19183117,\n",
       "        0.19180492, 0.19177918, 0.19175393, 0.19172918, 0.1917049 ,\n",
       "        0.1916811 , 0.19165775, 0.19163486, 0.19161241, 0.1915904 ,\n",
       "        0.19156881, 0.19154765, 0.19152689, 0.19150653, 0.19148657,\n",
       "        0.19146699, 0.1914478 , 0.19142897, 0.19141052, 0.19139241,\n",
       "        0.19137466, 0.19135726, 0.19134019, 0.19132345, 0.19130703,\n",
       "        0.19129094, 0.19127515, 0.19125967, 0.19124449, 0.19122961,\n",
       "        0.19121501, 0.1912007 , 0.19118666, 0.1911729 , 0.1911594 ,\n",
       "        0.19114616, 0.19113318, 0.19112045, 0.19110797, 0.19109573,\n",
       "        0.19108373, 0.19107196, 0.19106041, 0.1910491 , 0.191038  ,\n",
       "        0.19102711, 0.19101644, 0.19100597, 0.19099571, 0.19098564,\n",
       "        0.19097577, 0.19096609, 0.1909566 , 0.19094729, 0.19093817,\n",
       "        0.19092922, 0.19092044, 0.19091183, 0.19090339, 0.19089512,\n",
       "        0.190887  , 0.19087904, 0.19087124, 0.19086358, 0.19085608,\n",
       "        0.19084872, 0.1908415 , 0.19083442, 0.19082748, 0.19082068,\n",
       "        0.190814  , 0.19080746, 0.19080104, 0.19079475, 0.19078857,\n",
       "        0.19078252, 0.19077659, 0.19077077, 0.19076506, 0.19075946,\n",
       "        0.19075398, 0.19074859, 0.19074332, 0.19073814, 0.19073307,\n",
       "        0.19072809, 0.19072321, 0.19071842, 0.19071373, 0.19070913,\n",
       "        0.19070461, 0.19070019, 0.19069585, 0.19069159, 0.19068742,\n",
       "        0.19068333, 0.19067932, 0.19067538, 0.19067152, 0.19066774,\n",
       "        0.19066403, 0.19066039, 0.19065682, 0.19065332, 0.19064989,\n",
       "        0.19064652, 0.19064322, 0.19063999, 0.19063681, 0.1906337 ,\n",
       "        0.19063065, 0.19062766, 0.19062472, 0.19062184, 0.19061902,\n",
       "        0.19061625, 0.19061354, 0.19061088, 0.19060827, 0.19060571,\n",
       "        0.1906032 , 0.19060074, 0.19059833, 0.19059596, 0.19059364,\n",
       "        0.19059137, 0.19058913, 0.19058695, 0.1905848 , 0.1905827 ,\n",
       "        0.19058063, 0.19057861, 0.19057663, 0.19057468, 0.19057277,\n",
       "        0.1905709 , 0.19056907, 0.19056727, 0.1905655 , 0.19056377,\n",
       "        0.19056207, 0.19056041, 0.19055878, 0.19055718, 0.19055561,\n",
       "        0.19055407, 0.19055256, 0.19055108, 0.19054963, 0.19054821,\n",
       "        0.19054681, 0.19054545, 0.1905441 , 0.19054279, 0.1905415 ,\n",
       "        0.19054023, 0.19053899, 0.19053777, 0.19053658, 0.19053541,\n",
       "        0.19053426, 0.19053314, 0.19053204, 0.19053095, 0.19052989,\n",
       "        0.19052885, 0.19052783, 0.19052683, 0.19052585, 0.19052489,\n",
       "        0.19052394, 0.19052302, 0.19052211, 0.19052122, 0.19052035,\n",
       "        0.19051949, 0.19051866, 0.19051783, 0.19051703, 0.19051624,\n",
       "        0.19051546, 0.1905147 , 0.19051395, 0.19051322, 0.1905125 ,\n",
       "        0.1905118 , 0.19051111, 0.19051043, 0.19050977, 0.19050912,\n",
       "        0.19050848, 0.19050786, 0.19050724, 0.19050664, 0.19050605,\n",
       "        0.19050547, 0.19050491, 0.19050435, 0.1905038 , 0.19050327,\n",
       "        0.19050274, 0.19050223, 0.19050172, 0.19050123, 0.19050074,\n",
       "        0.19050027, 0.1904998 , 0.19049935, 0.1904989 , 0.19049846,\n",
       "        0.19049803, 0.1904976 , 0.19049719, 0.19049678, 0.19049638,\n",
       "        0.19049599, 0.19049561, 0.19049523, 0.19049486, 0.1904945 ,\n",
       "        0.19049415, 0.1904938 , 0.19049346, 0.19049312, 0.19049279,\n",
       "        0.19049247, 0.19049216, 0.19049185, 0.19049154, 0.19049125,\n",
       "        0.19049095, 0.19049067, 0.19049039, 0.19049011, 0.19048984,\n",
       "        0.19048958, 0.19048932, 0.19048907, 0.19048882, 0.19048857,\n",
       "        0.19048833, 0.1904881 , 0.19048787, 0.19048764, 0.19048742,\n",
       "        0.1904872 , 0.19048699, 0.19048678, 0.19048657, 0.19048637,\n",
       "        0.19048617, 0.19048598, 0.19048579, 0.1904856 , 0.19048542,\n",
       "        0.19048524, 0.19048507, 0.1904849 , 0.19048473, 0.19048456,\n",
       "        0.1904844 , 0.19048424, 0.19048409, 0.19048393, 0.19048378,\n",
       "        0.19048364, 0.19048349, 0.19048335, 0.19048321, 0.19048307,\n",
       "        0.19048294, 0.19048281, 0.19048268, 0.19048256, 0.19048243,\n",
       "        0.19048231, 0.19048219, 0.19048208, 0.19048196, 0.19048185,\n",
       "        0.19048174, 0.19048163, 0.19048153, 0.19048143, 0.19048132,\n",
       "        0.19048122, 0.19048113, 0.19048103, 0.19048094, 0.19048085,\n",
       "        0.19048076, 0.19048067, 0.19048058, 0.1904805 , 0.19048041,\n",
       "        0.19048033, 0.19048025, 0.19048017, 0.19048009, 0.19048002,\n",
       "        0.19047994, 0.19047987, 0.1904798 , 0.19047973, 0.19047966,\n",
       "        0.19047959, 0.19047953, 0.19047946, 0.1904794 , 0.19047934,\n",
       "        0.19047928, 0.19047922, 0.19047916, 0.1904791 , 0.19047904,\n",
       "        0.19047899, 0.19047894, 0.19047888, 0.19047883, 0.19047878,\n",
       "        0.19047873, 0.19047868, 0.19047863, 0.19047858, 0.19047854,\n",
       "        0.19047849, 0.19047845, 0.1904784 , 0.19047836, 0.19047832,\n",
       "        0.19047828, 0.19047824, 0.1904782 , 0.19047816, 0.19047812,\n",
       "        0.19047808, 0.19047805, 0.19047801, 0.19047798, 0.19047794,\n",
       "        0.19047791, 0.19047787, 0.19047784, 0.19047781, 0.19047778,\n",
       "        0.19047775, 0.19047772, 0.19047769, 0.19047766, 0.19047763,\n",
       "        0.1904776 , 0.19047757, 0.19047755, 0.19047752, 0.1904775 ,\n",
       "        0.19047747, 0.19047745, 0.19047742, 0.1904774 , 0.19047737,\n",
       "        0.19047735, 0.19047733, 0.19047731, 0.19047728, 0.19047726,\n",
       "        0.19047724, 0.19047722, 0.1904772 , 0.19047718, 0.19047716,\n",
       "        0.19047714, 0.19047713, 0.19047711, 0.19047709, 0.19047707,\n",
       "        0.19047706, 0.19047704, 0.19047702, 0.19047701, 0.19047699,\n",
       "        0.19047698, 0.19047696, 0.19047695, 0.19047693, 0.19047692,\n",
       "        0.1904769 , 0.19047689, 0.19047687, 0.19047686, 0.19047685,\n",
       "        0.19047684, 0.19047682, 0.19047681, 0.1904768 , 0.19047679,\n",
       "        0.19047678, 0.19047676, 0.19047675, 0.19047674, 0.19047673,\n",
       "        0.19047672, 0.19047671, 0.1904767 , 0.19047669, 0.19047668,\n",
       "        0.19047667, 0.19047666, 0.19047665, 0.19047664, 0.19047664,\n",
       "        0.19047663, 0.19047662, 0.19047661, 0.1904766 , 0.19047659,\n",
       "        0.19047659, 0.19047658, 0.19047657, 0.19047656, 0.19047656,\n",
       "        0.19047655, 0.19047654, 0.19047654, 0.19047653, 0.19047652,\n",
       "        0.19047652, 0.19047651, 0.1904765 , 0.1904765 , 0.19047649,\n",
       "        0.19047649, 0.19047648, 0.19047647, 0.19047647, 0.19047646,\n",
       "        0.19047646, 0.19047645, 0.19047645, 0.19047644, 0.19047644,\n",
       "        0.19047643, 0.19047643, 0.19047642, 0.19047642, 0.19047641,\n",
       "        0.19047641, 0.19047641, 0.1904764 , 0.1904764 , 0.19047639,\n",
       "        0.19047639, 0.19047639, 0.19047638, 0.19047638, 0.19047637,\n",
       "        0.19047637, 0.19047637, 0.19047636, 0.19047636, 0.19047636,\n",
       "        0.19047635, 0.19047635, 0.19047635, 0.19047635, 0.19047634,\n",
       "        0.19047634, 0.19047634, 0.19047633, 0.19047633, 0.19047633,\n",
       "        0.19047633, 0.19047632, 0.19047632, 0.19047632, 0.19047632,\n",
       "        0.19047631, 0.19047631, 0.19047631, 0.19047631, 0.1904763 ,\n",
       "        0.1904763 , 0.1904763 , 0.1904763 , 0.1904763 , 0.19047629,\n",
       "        0.19047629, 0.19047629, 0.19047629, 0.19047629, 0.19047628,\n",
       "        0.19047628, 0.19047628, 0.19047628, 0.19047628, 0.19047627,\n",
       "        0.19047627, 0.19047627, 0.19047627, 0.19047627, 0.19047627,\n",
       "        0.19047627, 0.19047626, 0.19047626, 0.19047626, 0.19047626,\n",
       "        0.19047626, 0.19047626, 0.19047626, 0.19047625, 0.19047625,\n",
       "        0.19047625, 0.19047625, 0.19047625, 0.19047625, 0.19047625,\n",
       "        0.19047625, 0.19047625, 0.19047624, 0.19047624, 0.19047624,\n",
       "        0.19047624, 0.19047624, 0.19047624, 0.19047624, 0.19047624,\n",
       "        0.19047624, 0.19047624, 0.19047623, 0.19047623, 0.19047623,\n",
       "        0.19047623, 0.19047623, 0.19047623, 0.19047623, 0.19047623,\n",
       "        0.19047623, 0.19047623, 0.19047623, 0.19047623, 0.19047623,\n",
       "        0.19047622, 0.19047622, 0.19047622, 0.19047622, 0.19047622,\n",
       "        0.19047622, 0.19047622, 0.19047622, 0.19047622, 0.19047622,\n",
       "        0.19047622, 0.19047622, 0.19047622, 0.19047622, 0.19047622,\n",
       "        0.19047622, 0.19047622, 0.19047622, 0.19047621, 0.19047621,\n",
       "        0.19047621, 0.19047621, 0.19047621, 0.19047621, 0.19047621,\n",
       "        0.19047621, 0.19047621, 0.19047621, 0.19047621, 0.19047621,\n",
       "        0.19047621, 0.19047621, 0.19047621, 0.19047621, 0.19047621,\n",
       "        0.19047621, 0.19047621, 0.19047621, 0.19047621, 0.19047621,\n",
       "        0.19047621, 0.19047621, 0.19047621, 0.19047621, 0.1904762 ,\n",
       "        0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 ,\n",
       "        0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 ,\n",
       "        0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 ,\n",
       "        0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 ,\n",
       "        0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 ,\n",
       "        0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 ,\n",
       "        0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 ,\n",
       "        0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 ,\n",
       "        0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 ,\n",
       "        0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 ,\n",
       "        0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 ,\n",
       "        0.1904762 , 0.1904762 , 0.1904762 , 0.1904762 , 0.19047619,\n",
       "        0.19047619, 0.19047619, 0.19047619, 0.19047619, 0.19047619,\n",
       "        0.19047619, 0.19047619, 0.19047619, 0.19047619, 0.19047619,\n",
       "        0.19047619, 0.19047619, 0.19047619, 0.19047619, 0.19047619,\n",
       "        0.19047619, 0.19047619, 0.19047619, 0.19047619, 0.19047619]),\n",
       " array([[ 3.03145324,  2.41944702],\n",
       "        [ 2.81549799,  1.71968554],\n",
       "        [ 2.7390515 ,  1.58518777],\n",
       "        ...,\n",
       "        [-1.42832489,  2.57136791],\n",
       "        [-1.42832729,  2.5713685 ],\n",
       "        [-1.42832967,  2.57136909]]))"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[1,2],[1,4],[1,5]])\n",
    "y = np.array([4,8,12])\n",
    "my_sgd = SGD(fit_intercept=True)\n",
    "my_sgd.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = np.array([[6,2],[2,4],[4,5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.71428571,  8.85714286, 11.42857143])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regr.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.42725279,  7.42881462,  7.14352025])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sgd.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
